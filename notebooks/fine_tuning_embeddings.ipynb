{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example to show how to use narration features in Ego4D.\n",
    "\n",
    "This notebook:\n",
    "1. Data Preparation\n",
    "    - data pre-processing\n",
    "    - couple two narration sets via nearest matching\n",
    "\n",
    "2. (Deliverable) Fine-tuned model\n",
    "    - filter out mismatched pairs (cos-sim < 0.5)\n",
    "    - train model in 1M narration pairs without splitting test set\n",
    "    - model saved in 'final_model_path'\n",
    "\n",
    "3. (Deliverable) Refined narration embeddings\n",
    "    - encode narrations from annotator1\n",
    "    - semantic search (compared with pre-trained embedding results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util, LoggingHandler, losses, InputExample\n",
    "import scipy.spatial as sp, scipy.cluster.hierarchy as hc\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, BinaryClassificationEvaluator, TripletEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "from multinegative_loss import MultipleNegativesLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "Load narration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NARRATION_PATH='/datasets01/ego4d_track2/v1/annotations/narration.json'\n",
    "narrations = json.load(open(NARRATION_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect narrations from two annotator sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_stamp_narrations_pass_1=[(uid,nar['timestamp_sec'],nar['timestamp_frame'],nar['narration_text']) for uid,value in narrations.items() if value['status']!='redacted' for nar in value['narration_pass_1']['narrations']]\n",
    "\n",
    "uid_stamp_narrations_pass_2=[(uid,nar['timestamp_sec'],nar['timestamp_frame'],nar['narration_text']) for uid,value in narrations.items() if value['status']!='redacted' for nar in value['narration_pass_2']['narrations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narration_uid_pass_1=np.array([uid for uid, _, _, _ in uid_stamp_narrations_pass_1])\n",
    "narration_stamp_sec_pass_1=np.array([stamp_sec for _, stamp_sec, _, _ in uid_stamp_narrations_pass_1])\n",
    "narration_stamp_frame_pass_1=np.array([stamp_frame for _, _, stamp_frame, _ in uid_stamp_narrations_pass_1])\n",
    "narration_text_pass_1=[text for _, _, _, text in uid_stamp_narrations_pass_1]\n",
    "\n",
    "narration_uid_pass_2=np.array([uid for uid, _, _, _ in uid_stamp_narrations_pass_2])\n",
    "narration_stamp_sec_pass_2=np.array([stamp_sec for _, stamp_sec, _, _ in uid_stamp_narrations_pass_2])\n",
    "narration_stamp_frame_pass_2=np.array([stamp_frame for _, _, stamp_frame, _ in uid_stamp_narrations_pass_2])\n",
    "narration_text_pass_2=[text for _, _, _, text in uid_stamp_narrations_pass_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dict_1 = defaultdict(dict)\n",
    "for uid, stamp_sec, stamp_frame, text in uid_stamp_narrations_pass_1:\n",
    "    dict_1[uid][stamp_frame]=text\n",
    "\n",
    "dict_2 = defaultdict(dict)\n",
    "for uid, stamp_sec, stamp_frame, text in uid_stamp_narrations_pass_2:\n",
    "    dict_2[uid][stamp_frame]=text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "couple narration pairs via nearest matching (within a stamp threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "# threshold: couple narrations if stamp_frame < threshold\n",
    "def collect_narration_pair(thresh):\n",
    "    output={}\n",
    "    for uid in set(narration_uid_pass_1).intersection(set(narration_uid_pass_2)):\n",
    "        sentence_pair={}\n",
    "\n",
    "        stamp_frame_1=narration_stamp_frame_pass_1[narration_uid_pass_1==uid]\n",
    "        stamp_frame_2=narration_stamp_frame_pass_2[narration_uid_pass_2==uid]\n",
    "\n",
    "        stamp_sec_1=narration_stamp_sec_pass_1[narration_uid_pass_1==uid]\n",
    "        stamp_sec_2=narration_stamp_sec_pass_2[narration_uid_pass_2==uid]\n",
    "\n",
    "        stamp_dist=distance_matrix(stamp_frame_1.reshape(-1,1),stamp_frame_2.reshape(-1,1))\n",
    "        stamp_frame_pair=[];stamp_sec_pair=[];narr_pair=[]\n",
    "        \n",
    "        while np.min(stamp_dist, axis=None)<=thresh: # find neareast frames under threshold\n",
    "\n",
    "            indx_1, indx_2=np.unravel_index(np.argmin(stamp_dist, axis=None), stamp_dist.shape)\n",
    "            stamp_frame_pair.append((stamp_frame_1[indx_1], stamp_frame_2[indx_2]))\n",
    "            stamp_sec_pair.append((stamp_sec_1[indx_1], stamp_sec_2[indx_2]))\n",
    "            narr_pair.append([dict_1[uid][stamp_frame_1[indx_1]],dict_2[uid][stamp_frame_2[indx_2]]])\n",
    "\n",
    "            stamp_dist[indx_1,:]=10000 # remove the paired sentences if matched\n",
    "            stamp_dist[:,indx_2]=10000\n",
    "\n",
    "        sentence_pair['stamp_frame_pair']=stamp_frame_pair\n",
    "        sentence_pair['stamp_sec_pair']=stamp_sec_pair\n",
    "        sentence_pair['narration_pair']=narr_pair\n",
    "\n",
    "        output[uid]=sentence_pair\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_data_thresh_20=collect_narration_pair(thresh=20) #set frame delta 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data\n",
    "f = open(\"narration_pair_data_thresh_20.pkl\",\"wb\") # create a binary pickle file \n",
    "pickle.dump(pair_data_thresh_20,f) # write the python object (dict) to pickle file\n",
    "f.close() # close file\n",
    "\n",
    "# reload data\n",
    "#with open('narration_pair_data_thresh_20.pkl', 'rb') as f:\n",
    "#    pair_data_thresh_20 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nar_comp=[n for k, v in pair_data_thresh_20.items() for f,n in zip(v['stamp_frame_pair'],v['narration_pair'])]\n",
    "df=pd.DataFrame(nar_comp).apply(lambda x: x.str.lower().replace({'^#[a-z]':'','^#\\s+[a-z]':'','^c\\s+c':'c','#unsure':'','#':'','\\s+':' '},regex=True).str.strip())\n",
    "df=df.apply(lambda x: x.str.lower().replace({'\\sc\\s':' person c ', '^c\\s':'person c ', '\\sc$':' person c', '\\.$':''},regex=True))\n",
    "pair_narration_all=df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nar_comp=[n for k, v in pair_data_thresh_20.items() for f,n in zip(v['stamp_frame_pair'],v['narration_pair']) if abs(f[1]-f[0])<=5]\n",
    "df=pd.DataFrame(nar_comp).apply(lambda x: x.str.lower().replace({'^#[a-z]':'','^#\\s+[a-z]':'','^c\\s+c':'c','#unsure':'','#':'','\\s+':' '},regex=True).str.strip())\n",
    "df=df.apply(lambda x: x.str.lower().replace({'\\sc\\s':' person c ', '^c\\s':'person c ', '\\sc$':' person c', '\\.$':''},regex=True))\n",
    "pair_narration_train=df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nar_comp=[n for k, v in pair_data_thresh_20.items() for f,n in zip(v['stamp_frame_pair'],v['narration_pair']) if (abs(f[1]-f[0])>5 and abs(f[1]-f[0])<=10)]\n",
    "df=pd.DataFrame(nar_comp).apply(lambda x: x.str.lower().replace({'^#[a-z]':'','^#\\s+[a-z]':'','^c\\s+c':'c','#unsure':'','#':'','\\s+':' '},regex=True).str.strip())\n",
    "df=df.apply(lambda x: x.str.lower().replace({'\\sc\\s':' person c ', '^c\\s':'person c ', '\\sc$':' person c', '\\.$':''},regex=True))\n",
    "pair_narration_test=df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size; stamp_sec delta\n",
    "print('frame_thresh: 20')\n",
    "len(pair_narration_all), max([abs(v_i[1]-v_i[0]) for k, v in pair_data_thresh_20.items() for v_i in v['stamp_sec_pair']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pair_narration_train), len(pair_narration_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calcuate consine similarity for each positive pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "sim_pre_trained_model=[]\n",
    "for i in range(len(pair_narration_all)):\n",
    "    pair_embed=pre_trained_model.encode(pair_narration_all[i])\n",
    "    sim_pre_trained_model.append(util.cos_sim(pair_embed[0],pair_embed[1]).item())\n",
    "\n",
    "# save result in txt\n",
    "output_file = open('cos_sim_rec/all_data_pre_trained_model.txt', 'w')\n",
    "for x in sim_pre_trained_model:\n",
    "    output_file.write(str(x) + '\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter out mismatched pairs if similarity less than a threshold (sim_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_thresh=0.5\n",
    "\n",
    "list_under_thresh=[1 if abs(f[1]-f[0])<=5 else 0 for k, v in pair_data_thresh_20.items() for f in v['stamp_frame_pair']]\n",
    "x=sum([list_under_thresh[i] for i in range(len(pair_narration_all)) if sim_pre_trained_model[i]>sim_thresh])\n",
    "print('when frame_delta= 5, # of samples < sim thresh: ',\n",
    "    x, 'out of', sum(list_under_thresh), 'is', 100*round(x/sum(list_under_thresh),4),'%'\n",
    "    )\n",
    "\n",
    "list_under_thresh=[1 if abs(f[1]-f[0])<=10 else 0 for k, v in pair_data_thresh_20.items() for f in v['stamp_frame_pair']]\n",
    "x=sum([list_under_thresh[i] for i in range(len(pair_narration_all)) if sim_pre_trained_model[i]>sim_thresh])\n",
    "print('when frame_delta=10, # of samples < sim thresh: ',\n",
    "    x, 'out of', sum(list_under_thresh), 'is', 100*round(x/sum(list_under_thresh),4),'%'\n",
    "    )\n",
    "\n",
    "list_under_thresh=[1 if abs(f[1]-f[0])<=15 else 0 for k, v in pair_data_thresh_20.items() for f in v['stamp_frame_pair']]\n",
    "x=sum([list_under_thresh[i] for i in range(len(pair_narration_all)) if sim_pre_trained_model[i]>sim_thresh])\n",
    "print('when frame_delta=15, # of samples < sim thresh: ',\n",
    "    x, 'out of', sum(list_under_thresh), 'is', 100*round(x/sum(list_under_thresh),4),'%'\n",
    "    )\n",
    "\n",
    "list_under_thresh=[1 if abs(f[1]-f[0])<=20 else 0 for k, v in pair_data_thresh_20.items() for f in v['stamp_frame_pair']]\n",
    "x=sum([list_under_thresh[i] for i in range(len(pair_narration_all)) if sim_pre_trained_model[i]>sim_thresh])\n",
    "print('when frame_delta=20, # of samples < sim thresh: ',\n",
    "    x, 'out of', sum(list_under_thresh), 'is', 100*round(x/sum(list_under_thresh),4),'%'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (Delierable) Fine-tuning all filtered samples without spliting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_thresh=0.5\n",
    "\n",
    "pair_narration_all_filtered=[pair_narration_all[i] for i in range(len(pair_narration_all)) if sim_pre_trained_model[i]>sim_thresh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split 20% training data as validate used to pick the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio=0.2\n",
    "\n",
    "pair_all_train_samples=[]\n",
    "pair_all_valid_samples=[]\n",
    "process_label=set(range(len(pair_narration_all_filtered)))\n",
    "for i in process_label:\n",
    "    pos_sample=pair_narration_all_filtered[i]\n",
    "    \n",
    "    np.random.seed(100*i+1)\n",
    "    rand_num=np.random.random(1) # generate sample in either train or validte set\n",
    "    if rand_num < valid_ratio:\n",
    "        random.seed(i)\n",
    "        neg_indx=random.sample(list(process_label-{i}),1)[0] # random select a negative pair\n",
    "        neg_sample=pair_narration_all_filtered[neg_indx]\n",
    "        inp_example = [\n",
    "        InputExample(texts=[pos_sample[0], pos_sample[1]], label=1), # one positive sample\n",
    "        InputExample(texts=[pos_sample[0], neg_sample[1]], label=0)  # one negative sample\n",
    "        ]\n",
    "        pair_all_valid_samples+=inp_example\n",
    "        \n",
    "    else:\n",
    "        inp_example = [\n",
    "        InputExample(texts=[pos_sample[0], pos_sample[1]]) # one positive sample\n",
    "        ]\n",
    "        pair_all_train_samples+=inp_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=8\n",
    "batch_size=32\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "evaluator0 = BinaryClassificationEvaluator.from_input_examples(pair_all_valid_samples, name='validation')\n",
    "train_dataloader = DataLoader(pair_all_train_samples, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "train_loss = MultipleNegativesLoss(model=model, neg_size=-1) #proportion of negatives; use (n-1) negatives if -1\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          evaluator=evaluator0,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          show_progress_bar=True,\n",
    "          output_path='final_model_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results=pd.read_csv(\"final_model_path/eval/binary_classification_evaluation_validation_results.csv\")\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.plot(np.array(range(len(validation_results)))*1000,validation_results.cossim_ap,label='ap')\n",
    "\n",
    "plt.plot(np.array(range(len(validation_results)))*1000,validation_results.cossim_accuracy,label='acc')\n",
    "\n",
    "plt.vlines(x=np.argmax(validation_results.cossim_ap)*1000,\n",
    "           ymin=min(validation_results.cossim_ap.min(),validation_results.cossim_accuracy.min()),\n",
    "           ymax=max(validation_results.cossim_ap.max(),validation_results.cossim_accuracy.max())+0.005,\n",
    "          color='red',linestyle='dotted')\n",
    "\n",
    "plt.xlabel('iteration steps')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (Deliverable) Narration Embeddings\n",
    "### 3.1 encode narrations from annotator1 as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_stamp_narrations=[(uid,nar['timestamp_sec'],nar['timestamp_frame'],nar['narration_text']) for uid,value in narrations.items() if value['status']!='redacted' for nar in value['narration_pass_1']['narrations']]\n",
    "\n",
    "narration_uid=[uid for uid, _, _, _ in uid_stamp_narrations]\n",
    "narration_stamp_sec=[stamp_sec for _, stamp_sec, _, _ in uid_stamp_narrations]\n",
    "narration_stamp_frame=[stamp_frame for _, _, stamp_frame, _ in uid_stamp_narrations]\n",
    "narration_text=[text for _, _, _, text in uid_stamp_narrations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(narration_text).apply(lambda x: x.str.lower().replace({'^#[a-z]':'','^#\\s+[a-z]':'','^c\\s+c':'c','#unsure':'','#':'','\\s+':' '},regex=True).str.strip())\n",
    "df=df.apply(lambda x: x.str.lower().replace({'\\sc\\s':' person c ', '^c\\s':'person c ', '\\sc$':' person c', '\\.$':''},regex=True))\n",
    "narration_text_proceeded=df.iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 semantic search (compared with pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = SentenceTransformer('final_model_path')\n",
    "\n",
    "corpus_embeddings = final_model.encode(narration_text_proceeded, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "pretrain_corpus_embeddings = pretrain_model.encode(narration_text_proceeded, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def corpus_search(model, query, corpus_embeddings, top_k):\n",
    "    query_embeddings = model.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_embeddings, corpus_embeddings, score_function=util.cos_sim, top_k=top_k) #topk narrations\n",
    "    \n",
    "    uid_dict = defaultdict(list) #aggregate the k narrations into a dict\n",
    "    for x in hits[0]:\n",
    "        uid=narration_uid[x['corpus_id']]  \n",
    "        uid_dict[uid].append( (narration_stamp_sec[x['corpus_id']],narration_text[x['corpus_id']],x['score']) )\n",
    "\n",
    "    return uid_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the search results from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['eating a meal']\n",
    "\n",
    "output=corpus_search(final_model, query, corpus_embeddings, 10000)\n",
    "\n",
    "print('# of videos with cosine similarity > 0.6: ',len([k for k,v in output.items() if v[0][2]>0.6]))\n",
    "\n",
    "[(k,v[0]) for k,v in output.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['eating a meal']\n",
    "\n",
    "output=corpus_search(pretrain_model, query, pretrain_corpus_embeddings, 10000)\n",
    "\n",
    "print('# of videos with cosine similarity > 0.6: ',len([k for k,v in output.items() if v[0][2]>0.6]))\n",
    "\n",
    "[(k,v[0]) for k,v in output.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['construction']\n",
    "\n",
    "output=corpus_search(final_model, query, corpus_embeddings, 10000)\n",
    "\n",
    "print('# of videos with cosine similarity > 0.6: ',len([k for k,v in output.items() if v[0][2]>0.6]))\n",
    "\n",
    "[(k,v[0]) for k,v in output.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['construction']\n",
    "\n",
    "output=corpus_search(pretrain_model, query, pretrain_corpus_embeddings, 10000)\n",
    "\n",
    "print('# of videos with cosine similarity > 0.6: ',len([k for k,v in output.items() if v[0][2]>0.6]))\n",
    "\n",
    "[(k,v[0]) for k,v in output.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['tree']\n",
    "\n",
    "output=corpus_search(final_model, query, corpus_embeddings, 10000)\n",
    "\n",
    "print('# of videos with cosine similarity > 0.6: ',len([k for k,v in output.items() if v[0][2]>0.6]))\n",
    "\n",
    "[(k,v[0]) for k,v in output.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['tree']\n",
    "\n",
    "output=corpus_search(pretrain_model, query, pretrain_corpus_embeddings, 10000)\n",
    "\n",
    "print('# of videos with cosine similarity > 0.6: ',len([k for k,v in output.items() if v[0][2]>0.6]))\n",
    "\n",
    "[(k,v[0]) for k,v in output.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
